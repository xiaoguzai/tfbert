import tensorflow as tf
import json
import tensorflow.keras.backend as K
import re
import tensorflow.keras as keras
import numpy as np
from tqdm import tqdm
config_path = '/home/xiaoguzai/æ•°æ®/unilm/mixed_corpus_bert_base_model/bert_config.json'
checkpoint_path = '/home/xiaoguzai/æ•°æ®/unilm/mixed_corpus_bert_base_model/bert_model.ckpt'
dict_path = '/home/xiaoguzai/æ•°æ®/unilm/mixed_corpus_bert_base_model/vocab.txt'

def is_equal(a,b):
    a = round(float(a),6)
    b = round(float(b),6)
    return a == b

def remove_bucket(equation):
    """å»æ‰å†—ä½™çš„æ‹¬å·
    """
    l_buckets, buckets = [], []
    for i, c in enumerate(equation):
        if c == '(':
            l_buckets.append(i)
        elif c == ')':
            buckets.append((l_buckets.pop(), i))
    eval_equation = eval(equation)
    for l, r in buckets:
        new_equation = '%s %s %s' % (
            equation[:l], equation[l + 1:r], equation[r + 1:]
        )
        try:
            if is_equal(eval(new_equation.replace(' ', '')), eval_equation):
                equation = new_equation
        except:
            pass
    return equation.replace(' ', '')
#å›å¤´å¯ä»¥å®éªŒä¸€ä¸‹ä¸å»æ‰å†—ä½™çš„æ‹¬å·ä»¥åŠä¸å°†ç©ºæ ¼æ›¿æ¢ä¸º''çš„å˜åŒ–

def load_data(filename):
    """è¯»å–è®­ç»ƒæ•°æ®ï¼Œå¹¶åšä¸€äº›æ ‡å‡†åŒ–ï¼Œä¿è¯equationæ˜¯å¯ä»¥evalçš„
    å‚è€ƒï¼šhttps://kexue.fm/archives/7809
    """
    D = []
    for l in open(filename):
        l = json.loads(l)
        question, equation, answer = l['original_text'], l['equation'], l['ans']
        # å¤„ç†å¸¦åˆ†æ•°,re.sub:å®ç°æ­£åˆ™çš„æ›¿æ¢
        # ()æ ‡è®°ä¸€ä¸ªå­è¡¨è¾¾å¼çš„å¼€å§‹å’Œç»“æŸä½ç½®ï¼Œå­è¡¨è¾¾å¼å¯ä»¥è·å–ä¾›ä»¥åä½¿ç”¨
        # è¦åŒ¹é…è¿™äº›å­—ç¬¦ï¼Œè¯·ä½¿ç”¨\(å’Œ\),æ‰€ä»¥è¯´(\d+)åé¢çš„\(ä»£è¡¨ç€å·¦åŠè¾¹çš„æ‹¬å·
        # (\d+/\d+)åé¢çš„\)ä»£è¡¨ç€å³åŠéƒ¨åˆ†çš„æ‹¬å·
        r"""
        æ•´ä¸ªæ›¿æ¢çš„è¿‡ç¨‹ï¼šå¯¹äºa%ç»Ÿä¸€æ›¿æ¢ä¸º(a/100),
        å¯¹äºa(b/c),ç»Ÿä¸€æ›¿æ¢ä¸º(a+b/c),
        å¯¹äº(a/b),å»æ‰æ‹¬å·å˜ä¸ºa/b,
        å¯¹äºæ¯”ä¾‹çš„å†’å·,ç»Ÿä¸€æ›¿æ¢ä¸º/ã€‚
        """
        question = re.sub('(\d+)\((\d+/\d+)\)', '(\\1+\\2)', question)
        equation = re.sub('(\d+)\((\d+/\d+)\)', '(\\1+\\2)', equation)
        answer = re.sub('(\d+)\((\d+/\d+)\)', '(\\1+\\2)', answer)
        #3(1/2)->(3+1/2)
        
        equation = re.sub('(\d+)\(', '\\1+(', equation)
        answer = re.sub('(\d+)\(', '\\1+(', answer)
        #38(1/2)->(38+1/2),(38(1/2))->(38+(1/2))
        
        question = re.sub('\((\d+/\d+)\)', '\\1', question)
        # å¤„ç†ç™¾åˆ†æ•°
        equation = re.sub('([\.\d]+)%', '(\\1/100)', equation)
        answer = re.sub('([\.\d]+)%', '(\\1/100)', answer)
        # questionè¿™é‡Œä¸å»é™¤åˆ†å·ï¼Ÿæ„Ÿè§‰å»é™¤åˆ†å·å¯èƒ½æ›´å¥½ä¸€ç‚¹
        # å†’å·è½¬é™¤å·ã€å‰©ä½™ç™¾åˆ†å·å¤„ç†
        equation = equation.replace(':', '/').replace('%', '/100')
        answer = answer.replace(':', '/').replace('%', '/100')
        if equation[:2] == 'x=':
            equation = equation[2:]
        #!!!å°ç»†èŠ‚éƒ¨åˆ†ï¼Œå‰é¢çš„'x='éœ€è¦è¢«å»æ‰
        try:
            if is_equal(eval(equation), eval(answer)):
                D.append((question, remove_bucket(equation), answer))
        except:
            #å¦‚æœeval(equation)å’Œeval(answer)çš„ç»“æœä¸ä¸€è‡´çš„æƒ…å†µä¸‹
            #è§†ä¸ºæ— æ•ˆå…¬å¼ï¼Œä¸æ”¾å…¥å¯¹åº”çš„Dæ•°ç»„ä¸­
            continue
    return D

# åŠ è½½æ•°æ®é›†
train_data = load_data('/home/xiaoguzai/æ•°æ®/data/train.ape.json')
valid_data = load_data('/home/xiaoguzai/æ•°æ®/data/valid.ape.json')
test_data = load_data('/home/xiaoguzai/æ•°æ®/data/test.ape.json')

from tokenization import load_vocab
#å½•å…¥æƒé‡çš„æ—¶å€™è¿˜æ˜¯éœ€è¦æ­£å¸¸åˆ‡è¯å¹¶å½•å…¥æƒé‡
#åªä¸è¿‡è¾“å‡ºçš„æ—¶å€™æƒé‡è¯å…¸å˜å¾—ç²¾ç®€äº†
token_dict = load_vocab(dict_path)
print('token_dict = ')
print(token_dict)

startswith = ['[PAD]','[UNK]','[CLS]','[SEP]']
new_token_dict,keep_tokens = {},[]
for t in startswith:
    new_token_dict[t] = len(new_token_dict)
    keep_tokens.append(token_dict[t])

from tokenization import FullTokenizer
for t, _ in sorted(token_dict.items(), key=lambda s: s[1]):
    if t not in new_token_dict:
        keep = True
        if len(t) > 1:
            for c in FullTokenizer.stem(t):
                if (
                    FullTokenizer._is_cjk_character(c) or
                    FullTokenizer._is_punctuation(c)
                ):
                    keep = False
                    break
        #æ³¨æ„å•ç‹¬ä½¿ç”¨!å’Œå‰é¢å¸¦æœ‰å‰ç¼€##!æ—¶çš„keepå¯¹åº”å€¼ä¸åŒ
        #å•ç‹¬ä½¿ç”¨!ä¸æ»¡è¶³if len(t) > 1,ç›´æ¥è¿›å…¥ä¸‹é¢çš„if keep
        #æ¡ä»¶åˆ¤å®šï¼Œè€Œå¦‚æœ##!æ»¡è¶³len(t) > 1,å°±ä¼šè¿›è¡Œæ¡ä»¶åˆ¤å®š
        #
        if keep:
            new_token_dict[t] = len(new_token_dict)
            keep_tokens.append(token_dict[t])
r"""new_token_dict = {'[PAD]':0,'[UNK]':1,'[CLS]':2,
'[SEP]':3,'!':4,'"':5,'#':6,...,'##ğŸ˜': 13583}
,len(keep_tokens) = 13584,len(new_token_dict) = 13584
"""

from models import Bert
from models import Embeddings
import tensorflow.keras as keras

tokenizer = FullTokenizer(vocab_file=new_token_dict)
#ä½¿ç”¨æ–°çš„new_token_dictå¯¹è¯­å¥è¿›è¡Œåˆ†è¯åˆ‡åˆ†
token_ids = []
segment_ids = []
for data in train_data:
    text1 = data[0]
    text2 = data[1]
    token1 = tokenizer.tokenize(text1)
    token2 = tokenizer.tokenize(text2)
    tokens = ["[CLS]"]+token1+["[SEP]"]+token2+["[SEP]"]
    token_id = tokenizer.convert_tokens_to_ids(tokens)
    token_ids.append(token_id)
    segment1 = [0]*(len(token1)+2)
    segment2 = [1]*(len(token2)+1)
    segment_id = segment1+segment2
    segment_ids.append(segment_id)

class CrossEntropy(tf.keras.layers.Layer):
    def __init__(self,**kwargs):
        super(CrossEntropy,self).__init__(**kwargs)
    
    def compute_loss(self,inputdata,y_pred):
        y_true = inputdata[0]
        y_mask = inputdata[0]
        loss = K.sparse_categorical_crossentropy(y_true,y_pred)
        y_mask = tf.cast(y_mask,dtype=tf.float32)
        loss = K.sum(tf.multiply(y_mask,loss))/K.sum(y_mask)
        #æƒ³è¦å°†å‰é¢çš„å•è¯å†…å®¹maskæ‰
        return loss
    
    def call(self,inputs):
        y_pred = inputs[1]
        inputdata = inputs[0]
        loss = self.compute_loss(inputdata,y_pred)
        self.add_loss(loss,inputs=inputs)
        return inputs

import json
json_file = '/home/xiaoguzai/æ•°æ®/unilm/mixed_corpus_bert_base_model/bert_config.json'
with open(json_file,'r') as load_f:
    load_dict = json.load(load_f)
    load_dict['hidden_dropout'] = load_dict['attention_probs_dropout_prob']
    load_dict['num_layers'] = load_dict['num_hidden_layers']
    load_dict['pooler_num_fc_layers'] = load_dict['pooler_fc_size']
    load_dict['embedding_size'] = load_dict['hidden_size']
    load_dict['vocab_size'] = len(new_token_dict)
    load_dict['embedding_size'] = load_dict['hidden_size']
    print(load_dict)

batch_size = 5
max_seq_len = 128
bertmodel = Bert(maxlen=max_seq_len,with_mlm=True,mode='unilm',
                solution='seq2seq',new_tokens=new_token_dict,**load_dict)
input_ids = [keras.layers.Input(shape=(None,),dtype='int32',name="token_ids"),
            keras.layers.Input(shape=(None,),dtype='int32',name="segment_ids")]
output = bertmodel(input_ids)
#ä½¿ç”¨ä¸€ä¸ªbertmodeldataè¿›è¡Œæµ‹è¯•å†…å®¹
#output = KerasTensor(shape=(None,128,30522),dtype=tf.float32)
output = CrossEntropy()([input_ids,output])
#è¿”å›çš„è¿˜æ˜¯å¸¸è§„çš„input
model = keras.models.Model(input_ids,output)
#ä¸Šé¢å†…å®¹ä½¿ç”¨seq2seqå¾ªç¯æ„å»ºæ¨¡å‹è®¡ç®—ç›¸åº”çš„æŸå¤±å†…å®¹
model.compile(optimizer=keras.optimizers.Adam())

#!!!!!!!!!!!!!!!!!!!!!è¿™é‡Œåº”è¯¥ä½¿ç”¨keras.models.Model(model.inputs,output)
#å¦åˆ™input_idsç›¸å½“äºè¢«å›ºå®šå¥½çš„å½¢çŠ¶ï¼Œåœ¨è¿™é‡Œä½¿ç”¨è‚¯å®šä¸è¡Œï¼Œåœ¨model.fitè®­ç»ƒçš„è¿‡ç¨‹ä¸­
#ä¼šè¢«ç›¸åº”çš„æŠ¥é”™ï¼Œæ‰€ä»¥è¿™é‡Œéœ€è¦æ ¹æ®è¾“å…¥è¿›è¡Œè°ƒæ•´

from loader import load_stock_weights
load_stock_weights(bert=bertmodel,new_tokens=keep_tokens,ckpt_path=checkpoint_path)

def sequence_padding(inputs,padding = 0):
    length = max([len(x) for x in inputs])
    pad_width = [(0,0) for _ in np.shape(inputs[0])]
    outputs = []
    for x in inputs:
        x = x[:length]
        pad_width[0] = (0,length-len(x))
        x = np.pad(x,pad_width,'constant',constant_values=padding)
        outputs.append(x)
    return outputs

class DataGenerator(object):
    def __init__(self,token_ids,segment_ids,batch_size=32,maxlen=128):
        self.token_ids = token_ids
        self.segment_ids = segment_ids
        self.batch_size = batch_size
        self.steps = int(np.floor(len(self.token_ids)/self.batch_size))
        self.totals = len(self.token_ids)
        self.maxlen = maxlen
    
    def __len__(self):
        return int(np.floor(len(self.token_ids)/self.batch_size))
    
    def sample(self, random=False):
        """é‡‡æ ·å‡½æ•°ï¼Œæ¯ä¸ªæ ·æœ¬åŒæ—¶è¿”å›ä¸€ä¸ªis_endæ ‡è®°
        """
        indices = list(range(len(self.token_ids)))
        np.random.shuffle(indices)
        for i in indices:
            yield self.token_ids[i],self.segment_ids[i]
        
    def __iter__(self,random=False):
        random = False
        batch_data = []
        batch_token_ids,batch_segment_ids = [],[]
        currents = 0
        for token_ids,segment_ids in self.sample(random):
        #ä¼ å…¥çš„æ•°æ®åœ¨ä¸‹é¢å®šä¹‰train_generator = data_generator(train_data, batch_size)
        #è¿™é‡Œå¦‚æœä½¿ç”¨tqdm(self.sample(random))ï¼Œå®ƒå°±ä¼šè¿ç»­åœ°ä¸æ–­äº§ç”Ÿçº¢è‰²åŒºåŸŸ
        #å¦‚æœä¸ä½¿ç”¨tqdm(self.sample(random))ï¼Œå®ƒå°±ä¼šè¿ç»­ä»¥...çš„å½¢å¼è¾“å‡ºè¿›åº¦
        #å› ä¸ºmodel.fit()å‡½æ•°ä¹‹ä¸­è‡ªå¸¦ç›¸åº”çš„è¿›åº¦æ¡
            if len(token_ids) > self.maxlen:
                token_ids = token_ids[:self.maxlen]
                segment_ids = segment_ids[:self.maxlen]
            batch_token_ids.append(token_ids)
            batch_segment_ids.append(segment_ids)
            currents = currents+1
            if len(batch_token_ids) == self.batch_size or currents == self.totals:
                #len(batch_token_ids) == self.batch_size:å½“å‰æ‰¹æ¬¡ç»“æŸ
                #is_end:æ‰€æœ‰æ•°æ®ç»“æŸ(å¯èƒ½ä¸å¤Ÿä¸€ä¸ªæ‰¹æ¬¡)
                batch_token_ids = sequence_padding(batch_token_ids)
                batch_segment_ids = sequence_padding(batch_segment_ids)
                yield [np.array(batch_token_ids),np.array(batch_segment_ids)]
                r"""
                è¿™é‡Œçš„batch_token_idså’Œbatch_segment_idså¤–é¢å¿…é¡»åŠ ä¸Šnp.array
                """
                batch_token_ids, batch_segment_ids = [], []
                batch_data = []
                #æ¯ä¸€ä¸ªæ‰¹æ¬¡ç»“æŸçš„æ—¶å€™

    def cycle(self,random=True):
        while True:
            for d in self.__iter__(random):
                yield d
r"""
è¿™é‡Œè¿”å›çš„d = ([array([[2,656,...,20,3],[2,2105,...,0,0],
...[2,569,...,0,0]]),array([[0,0,...,0,0,1,1],[0,0,...,0,0],
...[0,0,...,0,0]])],None)
è¿™é‡Œçš„å‰é¢ä¸€ä¸ªå±äºxï¼Œä»¥ä¸€ä¸ªå…ƒç»„çš„å½¢å¼è¾“å‡ºï¼Œåé¢ä¸€ä¸ªNoneä¸ºyï¼Œå› ä¸ºè¿™é‡Œ
æ˜¯seq2seqï¼Œæ‰€ä»¥ä¸éœ€è¦y

å¦‚æœä½¿ç”¨yield ([batch_token_ids,batch_segment_ids],None)çš„æ—¶å€™
æŠ¥é”™tuple index out of range
å¦‚æœä½¿ç”¨yield ([np.array(batch_token_ids),np.array(batch_segment_ids)
,None])çš„æ—¶å€™ï¼Œä¼šæŠ¥é”™  (0) Invalid argument:  required broadcastable shapes at loc(unknown)
 [[node model/bert/embeddings/add (defined at 
/home/xiaoguzai/ä»£ç /unilm-main/models.py:296) ]]
"""

class Evaluator(keras.callbacks.Callback):
    def __init__(self,topk,data,model):
        self.best_acc = 0
        self.maxlen = 64
        self.topk = topk
        self.data = data
        self.end_id = 3
        #self.end_id = '[SEP]' = 3,ä¸ºå¯¹åº”ç»“æŸæ ‡å¿—
        self.minlen = 1
        #self.minlen = 1,æœ€å°çš„é•¿åº¦
        self.min_ends = 1
        #self.min_ends = 1,ç»“æŸæ ‡å¿—çš„æœ€å°ä¸ªæ•°
        #è¿™é‡Œæ˜¯ç»Ÿè®¡self.end_idçš„ä¸ªæ•°çš„ï¼Œå› ä¸ºå¯èƒ½å½¢æˆ
        #çš„ç­”æ¡ˆä¹‹ä¸­æœ‰å¤šä¸ª'[SEP]'ï¼Œæ‰€ä»¥éœ€è¦å¯¹ç»“æœè¿›è¡Œ
        #ç»Ÿè®¡
        self.model = model
        
        #è¿™é‡Œä¼ å…¥çš„åº”è¯¥ä¸ºbertmodelçš„å¯¹åº”å†…å®¹
    
    def on_epoch_end(self,epoch=1,logs=None):
        metrics = self.evaluate(self.data,topk=self.topk)
        self.model.save_weights('./folder/best_model.weights')
        metrics['best_acc'] = self.best_acc
        print('valid_data:',metrics)
    
    def evaluate(self,data,topk=3):
        total,right = 0.0,0.0
        for question,equation,answer in tqdm(data):
            total = total+1
            token1 = tokenizer.tokenize(question)
            tokens = ["[CLS]"]+token1+["[SEP]"]
            token_ids = tokenizer.convert_tokens_to_ids(tokens)
            segment_ids = [0]*(len(tokens))
            #output_ids = self.beam_search([token_ids,segment_ids],topk=topk)
            output_ids = self.beam_search(token_ids,segment_ids,topk=topk)
            result_ids = tokenizer.convert_ids_to_tokens(output_ids)
            #ï¼ï¼ï¼è¯„ä¼°çš„è¿™é‡Œéœ€è¦ç”¨ä¸€ä¸ªæ–°çš„tokenizerè¿›è¡Œè¯„ä¼°ï¼Œå› ä¸ºè®­ç»ƒæ˜¯æŒ‰ç…§è¿™ä¸ªå­—å…¸åº
            #è®­ç»ƒçš„ï¼ï¼ï¼
            equation = ''.join(result_ids)
            equation = equation.replace(' ','')
            try:
                if is_equal(eval(equation),eval(answer)):
                    right = right+1
            except:
                pass
        return {'acc':right/total}
            
    def beam_search(self,token_id,segment_id,topk,states=None,temperature=1,min_ends=1):
        #æµ‹è¯•ï¼štopkä¸ºå…¶ä»–å€¼ï¼Œtopkä¸º0
        #ä¼ å…¥çš„æ˜¯å•ç»„çš„token_id,segment_id
        #token_id = [2,5,...102,99],segment_id = [0,0,...0,0]
        output_ids = [[]]
        token_ids = [token_id]
        segment_ids = [list(np.zeros(len(token_id)))]
        scores = [list(np.zeros(1))]
        current_token = token_ids
        current_segment = segment_ids
        #åˆå§‹åŒ–ä¸º0çš„æ—¶å€™æ–¹ä¾¿åé¢çš„åˆ†æ•°ä¸åˆå§‹åŒ–çš„output_scoresç›¸åŠ 
        for step in range(self.maxlen):
            output_scores = self.model([np.array(current_token),np.array(current_segment)])
            r"""
            è¿™é‡Œscoresçš„è¾“å‡ºå†…å®¹ä¸ºscores = [[array([[2,2008,...7941,3]],dtype=int32
            ),array([[0,0,...0,0]],dtype=int32)],array([[[2.76492733e-06,...
            2.77118647e-06]]],dtype=int32)]
            ä¸çŸ¥é“ä¸ºä»€ä¹ˆå‰é¢ä¸€ä¸ªæ•°ç»„ä¸­çš„ä¸¤ä¸ªarrayæ”¾å…¥äº†è¾“å…¥:[array([[2,2008,...7941,3]],
            dtype=int32)å’Œarray([[0,0,...0,0]],dtype=int32)],æœ€åä¸€ä¸ªæ•°ç»„å†…å®¹ä¸º
            array([[[2.76492733e-06,...2.77118647e-06],[2.76493734e-06,...
            2.77118352e-06],[2.76499100e-06,2.77118420e-06]]])
            ???ä¸çŸ¥é“ä¸ºä»€ä¹ˆè¾“å‡ºæ˜¯è¿™æ ·çš„ä¸€ç§å½¢å¼
            """
            #!!!è¿™é‡Œæ”¾å…¥çš„æ—¶å€™è¿˜å¾—å¿…é¡»æ˜¯np.arrayçš„å¯¹åº”æ•°ç»„ï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼Œ
            #æ˜å¤©è¿›å…¥æ¨¡å‹ä¹‹ä¸­æŸ¥çœ‹å¯¹åº”çš„list index out of rangeé”™è¯¯
            output_scores = output_scores[1]
            output_scores = output_scores.numpy()
            output_scores = output_scores[:,-1,:]
            #output_scores = (3,13584)
            output_scores = np.log(output_scores+1e-12)
            #!!!è¿™é‡Œçš„np.logæ¦‚ç‡å€¼æ˜¯ä¸ºäº†å¯¹åº”ä¹‹å‰çš„æŸå¤±å‡½æ•°
            #loss = K.sparse_categorical_crossentropy(y_true,y_pred)
            scores = output_scores+scores
            #ç°åœ¨çš„å¾—åˆ†åŠ ä¸Šä¹‹å‰çš„ç»¼åˆå¾—åˆ†
            indices = scores.argpartition(-topk,axis=None)[-topk:]
            #æ¯”å¦‚topk = 3,è¿›è¡Œåˆ°ç¬¬å››æ­¥çš„æ—¶å€™indices = (3,4)
            #æ­¤æ—¶ä»è¿™é‡Œé¢çš„3*4=12ä¸ªå…ƒç´ ä¹‹ä¸­å–å‡ºå‰3ä¸ªå…ƒç´ ï¼Œè¿™é‡Œaxis=None
            #å°±æ˜¯ä»æ•´ä¸ªå…ƒç´ å †ä¹‹ä¸­å–3ä¸ªæœ€å¥½çš„
            indices_1 = indices//scores.shape[1]
            indices_2 = (indices%scores.shape[1]).reshape((-1,1))
            #è®¡ç®—å‡ºå¯¹åº”çš„è¡Œåæ ‡indices_1å’Œåˆ—åæ ‡indices_2
            if step == 0:
                output_ids = np.array(indices_2)
            else:
                output_ids = np.concatenate([output_ids[indices_1],indices_2],1)
                #å¯¹åº”åˆ°ç›¸åº”çš„è¡Œå’Œåˆ—ç›¸åº”çš„idä¹‹ä¸­
            output_scores = np.take_along_axis(
                scores,indices,axis=None
            )
            #å°†ç°åœ¨çš„å…ƒç´ æ‹¼æ¥åˆ°å¯¹åº”çš„ä½ç½®ï¼Œå°†åŸå…ˆçš„(3,4)çŸ©é˜µå½¢æˆç°åœ¨çš„(3,5)çŸ©é˜µ

            end_counts = (output_ids == self.end_id).sum(1)
            #ç»Ÿè®¡å‡ºç°çš„endæ ‡è®°,æ³¨æ„è¿™é‡Œçš„output_idså¿…é¡»ä¸ºnp.array()ç±»å‹
            #è¾“å‡ºçš„æ‰èƒ½ä¸ºä¸€ä¸ªç›¸åº”çš„æ•°ç»„list
            if output_ids.shape[1] >= self.minlen:
            #output_ids.shape[1]ä¸ºå½“å‰å½¢æˆçš„å¥å­é•¿åº¦ï¼Œåœ¨å¥å­
            #é•¿åº¦å°äºself.minlençš„æƒ…å†µæ—¶ä¸è€ƒè™‘èˆå¼ƒç›¸åº”çš„åºåˆ—
            #ç­‰çš„ç›¸åº”çš„æ“ä½œï¼Œ>=self.minlençš„æƒ…å†µä¸‹éœ€è¦è€ƒè™‘åˆ°
                best_one = output_scores.argmax()
                #è·å–å¾—åˆ†æœ€å¤§çš„åºåˆ—
                if end_counts[best_one] == min_ends:
                #min_ends = 1ä¸ºç»“æŸçš„æœ€å°çš„æ ‡å¿—
                    return output_ids[best_one]
                #æ¦‚ç‡æœ€å¤§çš„end_countsæˆªæ­¢çš„æƒ…å†µä¸‹ï¼Œç›´æ¥è¿”å›
                else:
                    flag = (end_counts < min_ends)
                    if not flag.all():
                    #not flag.all()åˆ¤æ–­æ˜¯å¦å…¨éƒ¨å®Œæˆæ¯”è¾ƒå·§å¦™
                        inputs = [i[flag] for i in inputs]
                        output_ids = output_ids[flag]
                        output_scores = output_scores[flag]
                        end_counts = end_counts[flag]
                        #æ‰”æ‰å·²ç»å®Œæˆçš„åºåˆ—
                        topk = flag.sum()
                        r"""
                        output_ids = [[8105], [8105], [8105]]
                        output_scores = [0.01195427 0.01195427 0.01195427]
                        """
                #æ¦‚ç‡æœ€å¤§æœªæˆªæ­¢çš„æƒ…å†µä¸‹ï¼Œæ‰”æ‰æ¦‚ç‡è¾ƒå°å¹¶ä¸”è¾¾åˆ°æˆªæ­¢
                #æ ‡å¿—çš„æƒ…å†µ
    #ç»“æŸæƒ…å†µ1:è¾¾åˆ°æœ€å¤§çš„é•¿åº¦ã€‚2.æœ€å¤§çš„æ¦‚ç‡è¾¾åˆ°ç»“æŸæ ‡å¿—çš„é•¿åº¦
            current_token = np.concatenate([np.array([token_ids[0],token_ids[0],token_ids[0]]),output_ids],1)
            current_segment = np.concatenate([np.array([segment_ids[0],segment_ids[0],segment_ids[0]]),np.ones_like(output_ids)],1)
            #å»é™¤å®Œæˆä¹‹åå½¢æˆç›¸åº”çš„current_tokenå’Œcurrent_segmentå¯¹åº”çš„np.array()æ•°ç»„
        return output_ids[output_scores.argmax()]

train_generator = DataGenerator(token_ids,segment_ids,batch_size=32,maxlen=128)
topk = 3
evaluator = Evaluator(topk=topk,data=valid_data,model=model)
model.compile(optimizer=keras.optimizers.Adam())
model.fit(
    train_generator.cycle(),
    steps_per_epoch=len(train_generator),
    epochs=1,
    callbacks=[evaluator]
)

#model.fitä¸­é—´æ¨¡å‹çš„å¯¹åº”ç½‘ç»œå±‚å‚æ•°ä¸ä¼šè¢«è¾“å‡ºå‡ºæ¥çš„

r"""
æ¨¡å‹çš„è¾“å…¥å€¼åº”è¯¥ä¸º
input data = 
[array([[   2,  961,   20, ...,    0,    0,    0],
       ...,
       [   2, 3276, 4306, ...,    0,    0,    0]]), 
 array([[0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0]])]
"""

r"""
è¿™é‡Œä¸èƒ½ä½¿ç”¨åœ¨model.fitä¹‹ä¸­å®šä¹‰æŸå¤±å‡½æ•°çš„åŸå› åœ¨äº
è®¡ç®—lossçš„æ—¶å€™éœ€è¦ä½¿ç”¨input_idså’Œoutputçš„å†…å®¹
output = CrossEntropy([input_ids,output])
x:Input data.It could be:A numpy array
(or array-like),or a list of arrays
(in case the model has multiple inputs).
A generator or 'keras.utils.Sequence'
return '(inputs,targets)' or (inputs,targets
,sample_weights)
y:Target data.Like the input data 'x',
it could be either Numpy array(s) or TensorFlow
tensor(s).It should be consistent with 'x'(you
cannot have Numpy inputs and tensor targets,or
inversely).
If 'x' is a target,generator,or 'keras.utils.
Sequence' instance,'y' should not be specified
(since targets will be obtained from 'x').
"""
#len(train_generator)åœ¨ä¸Šé¢çš„def __len__(self)
#ä¹‹ä¸­å®šä¹‰è¿‡ï¼Œå¯ä»¥è¿”å›é•¿åº¦

#æ¯ä¸€æ¬¡seq2seqçš„è®­ç»ƒè¿‡ç¨‹ä¸­(x,y)å·²ç»åˆå¹¶åœ¨ä¸€èµ·äº†ï¼Œ
#æ‰€ä»¥seq2seqçš„è®­ç»ƒè¿‡ç¨‹åªæœ‰è¾“å…¥æ²¡æœ‰è¾“å‡ºï¼Œæ ¹æ®è¾“å…¥
#æ¥è®¡ç®—æ¯ä¸€ä¸ªæ¨¡å‹ä¹‹ä¸­çš„æƒé‡å‚æ•°

#(x,y)åœ¨seq2seqä¸­å®é™…ä¸ŠåˆäºŒä¸ºä¸€äº†ï¼Œæ‰€ä»¥è¿™é‡Œå¿…é¡»
#ä½¿ç”¨train_generator.cycle()å®ç°æ•°æ®çš„å¾ªç¯å¤„ç†