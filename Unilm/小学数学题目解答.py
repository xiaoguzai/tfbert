import tensorflow as tf
import json
import tensorflow.keras.backend as K
import re
import tensorflow.keras as keras
import numpy as np
from tqdm import tqdm
config_path = '/home/xiaoguzai/æ•°æ®/unilm/mixed_corpus_bert_base_model/bert_config.json'
checkpoint_path = '/home/xiaoguzai/æ•°æ®/unilm/mixed_corpus_bert_base_model/bert_model.ckpt'
dict_path = '/home/xiaoguzai/æ•°æ®/unilm/mixed_corpus_bert_base_model/vocab.txt'

def is_equal(a,b):
    a = round(float(a),6)
    b = round(float(b),6)
    return a == b

def remove_bucket(equation):
    """å»æ‰å†—ä½™çš„æ‹¬å·
    """
    l_buckets, buckets = [], []
    for i, c in enumerate(equation):
        if c == '(':
            l_buckets.append(i)
        elif c == ')':
            buckets.append((l_buckets.pop(), i))
    eval_equation = eval(equation)
    for l, r in buckets:
        new_equation = '%s %s %s' % (
            equation[:l], equation[l + 1:r], equation[r + 1:]
        )
        try:
            if is_equal(eval(new_equation.replace(' ', '')), eval_equation):
                equation = new_equation
        except:
            pass
    return equation.replace(' ', '')
#å›å¤´å¯ä»¥å®éªŒä¸€ä¸‹ä¸å»æ‰å†—ä½™çš„æ‹¬å·ä»¥åŠä¸å°†ç©ºæ ¼æ›¿æ¢ä¸º''çš„å˜åŒ–

def load_data(filename):
    """è¯»å–è®­ç»ƒæ•°æ®ï¼Œå¹¶åšä¸€äº›æ ‡å‡†åŒ–ï¼Œä¿è¯equationæ˜¯å¯ä»¥evalçš„
    å‚è€ƒï¼šhttps://kexue.fm/archives/7809
    """
    D = []
    for l in open(filename):
        l = json.loads(l)
        question, equation, answer = l['original_text'], l['equation'], l['ans']
        # å¤„ç†å¸¦åˆ†æ•°,re.sub:å®ç°æ­£åˆ™çš„æ›¿æ¢
        # ()æ ‡è®°ä¸€ä¸ªå­è¡¨è¾¾å¼çš„å¼€å§‹å’Œç»“æŸä½ç½®ï¼Œå­è¡¨è¾¾å¼å¯ä»¥è·å–ä¾›ä»¥åä½¿ç”¨
        # è¦åŒ¹é…è¿™äº›å­—ç¬¦ï¼Œè¯·ä½¿ç”¨\(å’Œ\),æ‰€ä»¥è¯´(\d+)åé¢çš„\(ä»£è¡¨ç€å·¦åŠè¾¹çš„æ‹¬å·
        # (\d+/\d+)åé¢çš„\)ä»£è¡¨ç€å³åŠéƒ¨åˆ†çš„æ‹¬å·
        r"""
        æ•´ä¸ªæ›¿æ¢çš„è¿‡ç¨‹ï¼šå¯¹äºa%ç»Ÿä¸€æ›¿æ¢ä¸º(a/100),
        å¯¹äºa(b/c),ç»Ÿä¸€æ›¿æ¢ä¸º(a+b/c),
        å¯¹äº(a/b),å»æ‰æ‹¬å·å˜ä¸ºa/b,
        å¯¹äºæ¯”ä¾‹çš„å†’å·,ç»Ÿä¸€æ›¿æ¢ä¸º/ã€‚
        """
        question = re.sub('(\d+)\((\d+/\d+)\)', '(\\1+\\2)', question)
        equation = re.sub('(\d+)\((\d+/\d+)\)', '(\\1+\\2)', equation)
        answer = re.sub('(\d+)\((\d+/\d+)\)', '(\\1+\\2)', answer)
        #3(1/2)->(3+1/2)
        
        equation = re.sub('(\d+)\(', '\\1+(', equation)
        answer = re.sub('(\d+)\(', '\\1+(', answer)
        #38(1/2)->(38+1/2),(38(1/2))->(38+(1/2))
        
        question = re.sub('\((\d+/\d+)\)', '\\1', question)
        # å¤„ç†ç™¾åˆ†æ•°
        equation = re.sub('([\.\d]+)%', '(\\1/100)', equation)
        answer = re.sub('([\.\d]+)%', '(\\1/100)', answer)
        # questionè¿™é‡Œä¸å»é™¤åˆ†å·ï¼Ÿæ„Ÿè§‰å»é™¤åˆ†å·å¯èƒ½æ›´å¥½ä¸€ç‚¹
        # å†’å·è½¬é™¤å·ã€å‰©ä½™ç™¾åˆ†å·å¤„ç†
        equation = equation.replace(':', '/').replace('%', '/100')
        answer = answer.replace(':', '/').replace('%', '/100')
        if equation[:2] == 'x=':
            equation = equation[2:]
        #!!!å°ç»†èŠ‚éƒ¨åˆ†ï¼Œå‰é¢çš„'x='éœ€è¦è¢«å»æ‰
        try:
            if is_equal(eval(equation), eval(answer)):
                D.append((question, remove_bucket(equation), answer))
        except:
            #å¦‚æœeval(equation)å’Œeval(answer)çš„ç»“æœä¸ä¸€è‡´çš„æƒ…å†µä¸‹
            #è§†ä¸ºæ— æ•ˆå…¬å¼ï¼Œä¸æ”¾å…¥å¯¹åº”çš„Dæ•°ç»„ä¸­
            continue
    return D

# åŠ è½½æ•°æ®é›†
train_data = load_data('/home/xiaoguzai/æ•°æ®/data/train.ape.json')
valid_data = load_data('/home/xiaoguzai/æ•°æ®/data/valid.ape.json')
test_data = load_data('/home/xiaoguzai/æ•°æ®/data/test.ape.json')

from tokenization import load_vocab
#å½•å…¥æƒé‡çš„æ—¶å€™è¿˜æ˜¯éœ€è¦æ­£å¸¸åˆ‡è¯å¹¶å½•å…¥æƒé‡
#åªä¸è¿‡è¾“å‡ºçš„æ—¶å€™æƒé‡è¯å…¸å˜å¾—ç²¾ç®€äº†
token_dict = load_vocab(dict_path)
print('token_dict = ')
print(token_dict)

startswith = ['[PAD]','[UNK]','[CLS]','[SEP]']
new_token_dict,keep_tokens = {},[]
for t in startswith:
    new_token_dict[t] = len(new_token_dict)
    keep_tokens.append(token_dict[t])

from tokenization import FullTokenizer
for t, _ in sorted(token_dict.items(), key=lambda s: s[1]):
    if t not in new_token_dict:
        keep = True
        if len(t) > 1:
            for c in FullTokenizer.stem(t):
                if (
                    FullTokenizer._is_cjk_character(c) or
                    FullTokenizer._is_punctuation(c)
                ):
                    keep = False
                    break
        #æ³¨æ„å•ç‹¬ä½¿ç”¨!å’Œå‰é¢å¸¦æœ‰å‰ç¼€##!æ—¶çš„keepå¯¹åº”å€¼ä¸åŒ
        #å•ç‹¬ä½¿ç”¨!ä¸æ»¡è¶³if len(t) > 1,ç›´æ¥è¿›å…¥ä¸‹é¢çš„if keep
        #æ¡ä»¶åˆ¤å®šï¼Œè€Œå¦‚æœ##!æ»¡è¶³len(t) > 1,å°±ä¼šè¿›è¡Œæ¡ä»¶åˆ¤å®š
        #
        if keep:
            new_token_dict[t] = len(new_token_dict)
            keep_tokens.append(token_dict[t])
r"""new_token_dict = {'[PAD]':0,'[UNK]':1,'[CLS]':2,
'[SEP]':3,'!':4,'"':5,'#':6,...,'##ğŸ˜': 13583}
,len(keep_tokens) = 13584,len(new_token_dict) = 13584
"""

print('new_token_dict = ')
print(new_token_dict)

from models import Bert
from models import Embeddings
import tensorflow.keras as keras

tokenizer = FullTokenizer(vocab_file=new_token_dict)
#ä½¿ç”¨æ–°çš„new_token_dictå¯¹è¯­å¥è¿›è¡Œåˆ†è¯åˆ‡åˆ†
token_ids = []
segment_ids = []
for data in train_data:
    text1 = data[0]
    text2 = data[1]
    token1 = tokenizer.tokenize(text1)
    token2 = tokenizer.tokenize(text2)
    tokens = ["[CLS]"]+token1+["[SEP]"]+token2+["[SEP]"]
    token_id = tokenizer.convert_tokens_to_ids(tokens)
    token_ids.append(token_id)
    segment1 = [0]*(len(token1)+2)
    segment2 = [1]*(len(token2)+1)
    segment_id = segment1+segment2
    segment_ids.append(segment_id)

class CrossEntropy(tf.keras.layers.Layer):
    def __init__(self,**kwargs):
        super(CrossEntropy,self).__init__(**kwargs)
    
    def compute_loss(self,inputdata,y_pred):
        y_true = inputdata[0]
        y_mask = inputdata[0]
        loss = K.sparse_categorical_crossentropy(y_true,y_pred)
        y_mask = tf.cast(y_mask,dtype=tf.float32)
        loss = K.sum(tf.multiply(y_mask,loss))/K.sum(y_mask)
        #æƒ³è¦å°†å‰é¢çš„å•è¯å†…å®¹maskæ‰
        return loss
    
    def call(self,inputs):
        y_pred = inputs[1]
        inputdata = inputs[0]
        loss = self.compute_loss(inputdata,y_pred)
        self.add_loss(loss,inputs=inputs)
        return inputs

import json
json_file = '/home/xiaoguzai/æ•°æ®/unilm/mixed_corpus_bert_base_model/bert_config.json'
with open(json_file,'r') as load_f:
    load_dict = json.load(load_f)
    load_dict['hidden_dropout'] = load_dict['attention_probs_dropout_prob']
    load_dict['num_layers'] = load_dict['num_hidden_layers']
    load_dict['pooler_num_fc_layers'] = load_dict['pooler_fc_size']
    load_dict['embedding_size'] = load_dict['hidden_size']
    load_dict['vocab_size'] = len(new_token_dict)
    load_dict['embedding_size'] = load_dict['hidden_size']
    print(load_dict)

batch_size = 5
max_seq_len = 128
bertmodel = Bert(maxlen=max_seq_len,with_mlm=True,mode='unilm',
                solution='seq2seq',new_tokens=new_token_dict,**load_dict)
input_ids = [keras.layers.Input(shape=(None,),dtype='int32',name="token_ids"),
            keras.layers.Input(shape=(None,),dtype='int32',name="segment_ids")]
output = bertmodel(input_ids)
#ä½¿ç”¨ä¸€ä¸ªbertmodeldataè¿›è¡Œæµ‹è¯•å†…å®¹
#output = KerasTensor(shape=(None,128,30522),dtype=tf.float32)
output = CrossEntropy()([input_ids,output])
#è¿”å›çš„è¿˜æ˜¯å¸¸è§„çš„input
model = keras.models.Model(input_ids,output)
#ä¸Šé¢å†…å®¹ä½¿ç”¨seq2seqå¾ªç¯æ„å»ºæ¨¡å‹è®¡ç®—ç›¸åº”çš„æŸå¤±å†…å®¹
model.compile(optimizer=keras.optimizers.Adam())

#!!!!!!!!!!!!!!!!!!!!!è¿™é‡Œåº”è¯¥ä½¿ç”¨keras.models.Model(model.inputs,output)
#å¦åˆ™input_idsç›¸å½“äºè¢«å›ºå®šå¥½çš„å½¢çŠ¶ï¼Œåœ¨è¿™é‡Œä½¿ç”¨è‚¯å®šä¸è¡Œï¼Œåœ¨model.fitè®­ç»ƒçš„è¿‡ç¨‹ä¸­
#ä¼šè¢«ç›¸åº”çš„æŠ¥é”™ï¼Œæ‰€ä»¥è¿™é‡Œéœ€è¦æ ¹æ®è¾“å…¥è¿›è¡Œè°ƒæ•´

from loader import load_stock_weights
load_stock_weights(bert=bertmodel,new_tokens=keep_tokens,ckpt_path=checkpoint_path)

def sequence_padding(inputs,padding = 0):
    length = max([len(x) for x in inputs])
    pad_width = [(0,0) for _ in np.shape(inputs[0])]
    outputs = []
    for x in inputs:
        x = x[:length]
        pad_width[0] = (0,length-len(x))
        x = np.pad(x,pad_width,'constant',constant_values=padding)
        outputs.append(x)
    return outputs

class DataGenerator(object):
    def __init__(self,token_ids,segment_ids,batch_size=32,maxlen=128):
        self.token_ids = token_ids
        self.segment_ids = segment_ids
        self.batch_size = batch_size
        self.steps = int(np.floor(len(self.token_ids)/self.batch_size))
        self.totals = len(self.token_ids)
        self.maxlen = maxlen
    
    def __len__(self):
        return int(np.floor(len(self.token_ids)/self.batch_size))
    
    def sample(self, random=False):
        """é‡‡æ ·å‡½æ•°ï¼Œæ¯ä¸ªæ ·æœ¬åŒæ—¶è¿”å›ä¸€ä¸ªis_endæ ‡è®°
        """
        indices = list(range(len(self.token_ids)))
        np.random.shuffle(indices)
        for i in indices:
            yield self.token_ids[i],self.segment_ids[i]
        
    def __iter__(self,random=False):
        random = False
        batch_data = []
        batch_token_ids,batch_segment_ids = [],[]
        currents = 0
        for token_ids,segment_ids in self.sample(random):
        #ä¼ å…¥çš„æ•°æ®åœ¨ä¸‹é¢å®šä¹‰train_generator = data_generator(train_data, batch_size)
        #è¿™é‡Œå¦‚æœä½¿ç”¨tqdm(self.sample(random))ï¼Œå®ƒå°±ä¼šè¿ç»­åœ°ä¸æ–­äº§ç”Ÿçº¢è‰²åŒºåŸŸ
        #å¦‚æœä¸ä½¿ç”¨tqdm(self.sample(random))ï¼Œå®ƒå°±ä¼šè¿ç»­ä»¥...çš„å½¢å¼è¾“å‡ºè¿›åº¦
        #å› ä¸ºmodel.fit()å‡½æ•°ä¹‹ä¸­è‡ªå¸¦ç›¸åº”çš„è¿›åº¦æ¡
            if len(token_ids) > self.maxlen:
                token_ids = token_ids[:self.maxlen]
                segment_ids = segment_ids[:self.maxlen]
            batch_token_ids.append(token_ids)
            batch_segment_ids.append(segment_ids)
            currents = currents+1
            if len(batch_token_ids) == self.batch_size or currents == self.totals:
                #len(batch_token_ids) == self.batch_size:å½“å‰æ‰¹æ¬¡ç»“æŸ
                #is_end:æ‰€æœ‰æ•°æ®ç»“æŸ(å¯èƒ½ä¸å¤Ÿä¸€ä¸ªæ‰¹æ¬¡)
                batch_token_ids = sequence_padding(batch_token_ids)
                batch_segment_ids = sequence_padding(batch_segment_ids)
                yield [np.array(batch_token_ids),np.array(batch_segment_ids)]
                r"""
                è¿™é‡Œçš„batch_token_idså’Œbatch_segment_idså¤–é¢å¿…é¡»åŠ ä¸Šnp.array
                """
                batch_token_ids, batch_segment_ids = [], []
                batch_data = []
                #æ¯ä¸€ä¸ªæ‰¹æ¬¡ç»“æŸçš„æ—¶å€™

    def cycle(self,random=True):
        while True:
            for d in self.__iter__(random):
                yield d
r"""
è¿™é‡Œè¿”å›çš„d = ([array([[2,656,...,20,3],[2,2105,...,0,0],
...[2,569,...,0,0]]),array([[0,0,...,0,0,1,1],[0,0,...,0,0],
...[0,0,...,0,0]])],None)
è¿™é‡Œçš„å‰é¢ä¸€ä¸ªå±äºxï¼Œä»¥ä¸€ä¸ªå…ƒç»„çš„å½¢å¼è¾“å‡ºï¼Œåé¢ä¸€ä¸ªNoneä¸ºyï¼Œå› ä¸ºè¿™é‡Œ
æ˜¯seq2seqï¼Œæ‰€ä»¥ä¸éœ€è¦y

å¦‚æœä½¿ç”¨yield ([batch_token_ids,batch_segment_ids],None)çš„æ—¶å€™
æŠ¥é”™tuple index out of range
å¦‚æœä½¿ç”¨yield ([np.array(batch_token_ids),np.array(batch_segment_ids)
,None])çš„æ—¶å€™ï¼Œä¼šæŠ¥é”™  (0) Invalid argument:  required broadcastable shapes at loc(unknown)
 [[node model/bert/embeddings/add (defined at 
/home/xiaoguzai/ä»£ç /unilm-main/models.py:296) ]]
"""

from Evaluator import Evaluator

train_generator = DataGenerator(token_ids,segment_ids,batch_size=32,maxlen=128)
topk = 3
evaluator = Evaluator(topk=topk,data=valid_data,model=model)
evaluator.on_epoch_end()
